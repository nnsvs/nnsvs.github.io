<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>How to train neural vocoders with ParallelWaveGAN &mdash; nnsvs 0.1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/plot_directive.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=01f34227"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="How to train uSFGAN/SiFiGAN vocoders" href="train_usfgan.html" />
    <link rel="prev" title="Hyperparameter optimization with Optuna" href="optuna.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            nnsvs
          </a>
              <div class="version">
                0.1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Demos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/Demos.html">NNSVS demos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes.html">Getting started with recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_choose_model.html">How to choose model</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_models.html">Defining your custom model</a></li>
<li class="toctree-l1"><a class="reference internal" href="devdocs.html">Development guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="update_guide.html">Update guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="optuna.html">Hyperparameter optimization with Optuna</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to train neural vocoders with ParallelWaveGAN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pre-requisites">Pre-requisites</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#what-is-nsf">What is NSF?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#input-output-of-a-neural-vocoder">Input/output of a neural vocoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-a-fork-of-parallel-wavegan">Install a fork of parallel_wavegan</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vocoder-settings-in-config-yaml">Vocoder settings in <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#stage-9-prepare-features-for-neural-vocoders">Stage 9: Prepare features for neural vocoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stage-10-training-vocoder-using-parallel-wavegan">Stage 10: Training vocoder using parallel_wavegan</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stage-11-synthesis-waveforms-by-parallel-wavegan">Stage 11: Synthesis waveforms by parallel_wavegan</a></li>
<li class="toctree-l2"><a class="reference internal" href="#packing-models-with-neural-vocoder">Packing models with neural vocoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-use-the-packed-model-with-the-trained-vocoder">How to use the packed model with the trained vocoder?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#available-neural-vocoders">Available neural vocoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-train-universal-vocoders">How to train universal vocoders?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#training-on-mixed-singing-databases">Training on mixed singing databases</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-on-mixed-speech-and-singing-databases">Training on mixed speech and singing databases</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="train_usfgan.html">How to train uSFGAN/SiFiGAN vocoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="enunu2nnsvs.html">How to convert ENUNU models to NNSVS’ ones</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview of NNSVS’s SVS</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules/base.html">nnsvs.base</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/model.html">nnsvs.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/acoustic_models.html">nnsvs.acoustic_models</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/postfilters.html">nnsvs.postfilters</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/discriminators.html">nnsvs.discriminators</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/pretrained.html">nnsvs.pretrained</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/svs.html">nnsvs.svs</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/dsp.html">nnsvs.dsp</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/gen.html">nnsvs.gen</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/io.html">nnsvs.io</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/mdn.html">nnsvs.mdn</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/pitch.html">nnsvs.pitch</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/multistream.html">nnsvs.multistream</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/util.html">nnsvs.util</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/train_util.html">nnsvs.train_util</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="links.html">Useful links</a></li>
<li class="toctree-l1"><a class="reference internal" href="papers.html">Papers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Meta information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Change log</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">nnsvs</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">How to train neural vocoders with ParallelWaveGAN</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/train_vocoders.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="how-to-train-neural-vocoders-with-parallelwavegan">
<h1>How to train neural vocoders with ParallelWaveGAN<a class="headerlink" href="#how-to-train-neural-vocoders-with-parallelwavegan" title="Link to this heading"></a></h1>
<p>Please check <a class="reference internal" href="recipes.html"><span class="doc">Getting started with recipes</span></a> and <a class="reference internal" href="overview.html"><span class="doc">Overview of NNSVS’s SVS</span></a> first.</p>
<p>NNSVS v0.0.3 and later supports neural vocoders. This page summarizes how to train neural vocoders.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This section needs to be re-written according to the recent major updates. Will be updated soon.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The contents in this page is based on <code class="docutils literal notranslate"><span class="pre">recipes/conf/spsvs/run_common_steps_dev.sh</span></code>.
Also, before you make your custom recipes, it is recommenced to start with a test recipe <code class="docutils literal notranslate"><span class="pre">recipes/nit-song070/dev-test</span></code>.</p>
</div>
<section id="pre-requisites">
<h2>Pre-requisites<a class="headerlink" href="#pre-requisites" title="Link to this heading"></a></h2>
<section id="what-is-nsf">
<h3>What is NSF?<a class="headerlink" href="#what-is-nsf" title="Link to this heading"></a></h3>
<p>NSF is an abbreviation of a neural source-filter model for speech synthesis (<span id="id1">Wang <em>et al.</em> [<a class="reference internal" href="papers.html#id56" title="Xin Wang, Shinji Takaki, and Junichi Yamagishi. Neural source-filter waveform models for statistical parametric speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:402–415, 2019.">WTY19</a>]</span>).</p>
</section>
<section id="input-output-of-a-neural-vocoder">
<h3>Input/output of a neural vocoder<a class="headerlink" href="#input-output-of-a-neural-vocoder" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Input: acoustic features containing only static features</p></li>
<li><p>Output: waveform</p></li>
</ul>
<p>The current implementation of NNSVS does not use dynamic features (i.e., delta and delta-delta features) for neural vocoders.
If you enabled dynamic features, you must need to extract static features before training neural vocoders.</p>
</section>
<section id="install-a-fork-of-parallel-wavegan">
<h3>Install a fork of parallel_wavegan<a class="headerlink" href="#install-a-fork-of-parallel-wavegan" title="Link to this heading"></a></h3>
<p>NNSVS’s neural vocoder integration is done by the external <a class="reference external" href="https://github.com/r9y9/ParallelWaveGAN">r9y9/parallel_wavegan</a> repository that provides various GAN-based neural vocoders.
To enable the neural vocoder support for NNSVS, you must need to install the nnsvs branch of the parallel_wavegan repository in advance:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">r9y9</span><span class="o">/</span><span class="n">ParallelWaveGAN</span><span class="nd">@nnsvs</span>
</pre></div>
</div>
<p>The nnsvs branch includes code for NSF.
After installation, please make sure that you can access <code class="docutils literal notranslate"><span class="pre">HnSincNSF</span></code> class. The following code should throw no errors if the installtaion is property done.</p>
<p>Jupyter:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="kn">from</span> <span class="nn">parallel_wavegan.models.nsf</span> <span class="kn">import</span> <span class="n">HnSincNSF</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">2</span><span class="p">]:</span> <span class="n">HnSincNSF</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Command-line:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;from parallel_wavegan.models.nsf import HnSincNSF; print(HnSincNSF(1,1))&quot;</span>
</pre></div>
</div>
</section>
<section id="vocoder-settings-in-config-yaml">
<h3>Vocoder settings in <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code><a class="headerlink" href="#vocoder-settings-in-config-yaml" title="Link to this heading"></a></h3>
<p>The following settings are related to neural vocoders:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># NOTE: conf/parallel_wavegan/${vocoder_model}.yaml must exist.</span>
<span class="n">vocoder_model</span><span class="p">:</span> <span class="n">hn</span><span class="o">-</span><span class="n">sinc</span><span class="o">-</span><span class="n">nsf_sr48k_pwgD_test</span>
<span class="c1"># Pretrained checkpoint path for the vocoder model</span>
<span class="c1"># NOTE: if you want to try fine-tuning, please specify the path here</span>
<span class="n">pretrained_vocoder_checkpoint</span><span class="p">:</span>
<span class="c1"># absolute/relative path to the checkpoint</span>
<span class="c1"># NOTE: the checkpoint is used for synthesis and packing</span>
<span class="c1"># This doesn&#39;t have any effect on training</span>
<span class="n">vocoder_eval_checkpoint</span><span class="p">:</span>
</pre></div>
</div>
<p>You can manually edit them or set them by command line like <code class="docutils literal notranslate"><span class="pre">---vocoder-model</span> <span class="pre">hn-sinc-nsf_sr48k_pwgD_test</span></code>.</p>
</section>
</section>
<section id="stage-9-prepare-features-for-neural-vocoders">
<h2>Stage 9: Prepare features for neural vocoders<a class="headerlink" href="#stage-9-prepare-features-for-neural-vocoders" title="Link to this heading"></a></h2>
<p>The pre-processing for neural vocoders (i.e., extract static features from acoustic features) is implemented as stage 9.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>./run.sh<span class="w"> </span>--stage<span class="w"> </span><span class="m">9</span><span class="w"> </span>--stop-stage<span class="w"> </span><span class="m">9</span>
</pre></div>
</div>
<p>After running the above command, you will see the following output:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Converting dump/yoko/norm/out_acoustic_scaler.joblib mean/scale npy files
[mean] dim: (206,) -&gt; (67,)
[scale] dim: (206,) -&gt; (67,)
[var] dim: (206,) -&gt; (67,)

If you are going to train NSF-based vocoders, please set the following parameters:

out_lf0_mean: 5.9012025218118325
out_lf0_scale: 0.2378365181913869

NOTE: If you are using the same data for training acoustic/vocoder models, the F0 statistics
for those models should be the same. If you are using different data for training
acoustic/vocoder models (e.g., training a vocoder model on a multiple DBs),
you will likely need to set different F0 statistics for acoustic/vocoder models.
</pre></div>
</div>
<p>After the pre-processing is property done, you can find the all the necessary features for training neural vocoders:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>tree -L 3 dump/yoko/

dump/yoko/
├── norm
│   ├── dev
│   │   ├── in_acoustic
│   │   ├── in_duration
│   │   ├── in_timelag
│   │   ├── in_vocoder
│   │   ├── out_acoustic
│   │   ├── out_duration
│   │   ├── out_postfilter
│   │   └── out_timelag
│   ├── eval
│   │   ├── in_acoustic
│   │   ├── in_duration
│   │   ├── in_timelag
│   │   ├── in_vocoder
│   │   ├── out_acoustic
│   │   ├── out_duration
│   │   ├── out_postfilter
│   │   └── out_timelag
│   ├── in_acoustic_scaler.joblib
│   ├── in_duration_scaler.joblib
│   ├── in_timelag_scaler.joblib
│   ├── in_vocoder_scaler_mean.npy
│   ├── in_vocoder_scaler_scale.npy
│   ├── in_vocoder_scaler_var.npy
│   ├── out_acoustic_scaler.joblib
│   ├── out_duration_scaler.joblib
│   ├── out_postfilter_scaler.joblib
│   ├── out_timelag_scaler.joblib
│   └── train_no_dev
│       ├── in_acoustic
│       ├── in_duration
│       ├── in_timelag
│       ├── in_vocoder
│       ├── out_acoustic
│       ├── out_duration
│       ├── out_postfilter
│       └── out_timelag
</pre></div>
</div>
<p>Some notes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">norm/${spk}/*/in_vocoder</span></code> directory contains features for neural vocoders. Note that the directory contains both the input and output features. Specifically, <code class="docutils literal notranslate"><span class="pre">*-feats.npy</span></code> contains static features consisting of <code class="docutils literal notranslate"><span class="pre">mgc</span></code>, <code class="docutils literal notranslate"><span class="pre">lf0</span></code>, <code class="docutils literal notranslate"><span class="pre">vuv</span></code> and <code class="docutils literal notranslate"><span class="pre">bap</span></code>; <code class="docutils literal notranslate"><span class="pre">*-wave.npy</span></code> contains raw waveform, respectively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">norm/in_vocoder_scaler_*.npy</span></code> contains statistics used to normalize/de-normalize the input features for neural vocoders.</p></li>
</ul>
</section>
<section id="stage-10-training-vocoder-using-parallel-wavegan">
<h2>Stage 10: Training vocoder using parallel_wavegan<a class="headerlink" href="#stage-10-training-vocoder-using-parallel-wavegan" title="Link to this heading"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You must configure vocoder configs according to the sampling rate of the waveform and your feature extraction settings. It is strongly recommenced to go though the vocoder config before training your model. Vocoder configs for 24khz and 48kHz are available in the NNSVS repository, but can be extended for other sampling rates (e.g., 16kHz).</p>
</div>
<p>Once the pre-processing is done, you can train a neural vocoder by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>./run.sh<span class="w"> </span>--stage<span class="w"> </span><span class="m">10</span><span class="w"> </span>--stop-stage<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vocoder-model<span class="w"> </span>hn-sinc-nsf_sr48k_pwgD_test
</pre></div>
</div>
<p>You can find available model configs in <code class="docutils literal notranslate"><span class="pre">conf/parlalel_wavegan</span></code> directory, or you can create your own model config. Please do make sure to set <code class="docutils literal notranslate"><span class="pre">out_lf0_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">out_lf0_scale</span></code> parameters correctly.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ tree conf/parallel_wavegan
conf/parallel_wavegan
├── hn-sinc-nsf_sr24k_pwgD.yaml
├── hn-sinc-nsf_sr48k_hifiganD.yaml
├── hn-sinc-nsf_sr48k_pwgD.yaml
└── hn-sinc-nsf_sr48k_pwgD_test.yaml
</pre></div>
</div>
<p>Training progress can be monitored by tensorboard. During training you can check generated waveforms in <code class="docutils literal notranslate"><span class="pre">exp/${speaker</span> <span class="pre">name}/${vocoder</span> <span class="pre">config</span> <span class="pre">name}/predictions</span></code> directory.</p>
</section>
<section id="stage-11-synthesis-waveforms-by-parallel-wavegan">
<h2>Stage 11: Synthesis waveforms by parallel_wavegan<a class="headerlink" href="#stage-11-synthesis-waveforms-by-parallel-wavegan" title="Link to this heading"></a></h2>
<p>Stage 11 generates waveforms using the trained neural vocoder. Please make sure to specify your model type explicitly.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>./run.sh<span class="w"> </span>--stage<span class="w"> </span><span class="m">11</span><span class="w"> </span>--stop-stage<span class="w"> </span><span class="m">11</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vocoder-model<span class="w"> </span>hn-sinc-nsf_sr48k_pwgD_test
</pre></div>
</div>
<p>Generated wav files can be found in <code class="docutils literal notranslate"><span class="pre">exp/${speaker</span> <span class="pre">name}/${vocoder</span> <span class="pre">config</span> <span class="pre">name}/wav</span></code> directory.
To generate waveforms from a specific checkpoint, please specify the checkpoint path by <code class="docutils literal notranslate"><span class="pre">--vocoder-eval-checkpoint</span> <span class="pre">/path/to/checkpoint</span></code>.</p>
</section>
<section id="packing-models-with-neural-vocoder">
<h2>Packing models with neural vocoder<a class="headerlink" href="#packing-models-with-neural-vocoder" title="Link to this heading"></a></h2>
<p>To package all the models together, you can run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>./run.sh<span class="w"> </span>--stage<span class="w"> </span><span class="m">99</span><span class="w"> </span>--stop-stage<span class="w"> </span><span class="m">99</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timelag-model<span class="w"> </span>timelag_test<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--duration-model<span class="w"> </span>duration_test<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--acoustic-model<span class="w"> </span>acoustic_test<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vocoder_model<span class="w"> </span>hn-sinc-nsf_sr48k_pwgD_test
</pre></div>
</div>
<p>Please make sure to add <code class="docutils literal notranslate"><span class="pre">--vocoder_model</span> <span class="pre">${vocoder</span> <span class="pre">config</span> <span class="pre">name}</span></code> to package the trained vocoder as well.
You can also specify the explicit path of the trained model by <code class="docutils literal notranslate"><span class="pre">--vocoder-eval-checkpoint</span> <span class="pre">/path/to/checkpoint</span></code>.</p>
</section>
<section id="how-to-use-the-packed-model-with-the-trained-vocoder">
<h2>How to use the packed model with the trained vocoder?<a class="headerlink" href="#how-to-use-the-packed-model-with-the-trained-vocoder" title="Link to this heading"></a></h2>
<p>Please specify <code class="docutils literal notranslate"><span class="pre">vocder_type=&quot;pwg&quot;</span></code> with the <a class="reference internal" href="modules/svs.html"><span class="doc">nnsvs.svs</span></a> module. An example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pysinsy</span>
<span class="kn">from</span> <span class="nn">nnmnkwii.io</span> <span class="kn">import</span> <span class="n">hts</span>
<span class="kn">from</span> <span class="nn">nnsvs.pretrained</span> <span class="kn">import</span> <span class="n">retrieve_pretrained_model</span>
<span class="kn">from</span> <span class="nn">nnsvs.svs</span> <span class="kn">import</span> <span class="n">SPSVS</span>
<span class="kn">from</span> <span class="nn">nnsvs.util</span> <span class="kn">import</span> <span class="n">example_xml_file</span>

<span class="n">model_dir</span> <span class="o">=</span> <span class="s2">&quot;/path/to/your/packed/model_dir&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">SPSVS</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>

<span class="n">contexts</span> <span class="o">=</span> <span class="n">pysinsy</span><span class="o">.</span><span class="n">extract_fullcontext</span><span class="p">(</span><span class="n">example_xml_file</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;get_over&quot;</span><span class="p">))</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">hts</span><span class="o">.</span><span class="n">HTSLabelFile</span><span class="o">.</span><span class="n">create_from_contexts</span><span class="p">(</span><span class="n">contexts</span><span class="p">)</span>

<span class="n">wav</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">svs</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">vocoder_type</span><span class="o">=</span><span class="s2">&quot;pwg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="available-neural-vocoders">
<h2>Available neural vocoders<a class="headerlink" href="#available-neural-vocoders" title="Link to this heading"></a></h2>
<p>In addition to NSF, <em>any</em> models implemented in <a class="reference external" href="https://github.com/kan-bayashi/ParallelWaveGAN">parallel_wavegan</a> can be used with NNSVS. For example, Parallel WaveGAN, HiFiGAN, MelGAN, etc.
However, to get the best performance in singing synthesis, I’d recommend using <code class="docutils literal notranslate"><span class="pre">HnSincNSF</span></code> model (<span id="id2">Wang and Yamagishi [<a class="reference internal" href="papers.html#id62" title="Xin Wang and Junichi Yamagishi. Neural harmonic-plus-noise waveform model with trainable maximum voice frequency for text-to-speech synthesis. arXiv preprint arXiv:1908.10256, 2019.">WY19</a>]</span>), which is an advanced version of the original NSF (<span id="id3">Wang <em>et al.</em> [<a class="reference internal" href="papers.html#id56" title="Xin Wang, Shinji Takaki, and Junichi Yamagishi. Neural source-filter waveform models for statistical parametric speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:402–415, 2019.">WTY19</a>]</span>).</p>
</section>
<section id="how-to-train-universal-vocoders">
<h2>How to train universal vocoders?<a class="headerlink" href="#how-to-train-universal-vocoders" title="Link to this heading"></a></h2>
<p>It is possible to make an <em>universal</em> vocoder that generalizes well on unseen speaker’s data by training a vocoder on a large amount of mixed databases.</p>
<section id="training-on-mixed-singing-databases">
<h3>Training on mixed singing databases<a class="headerlink" href="#training-on-mixed-singing-databases" title="Link to this heading"></a></h3>
<p>Suppose you have multiple singing databases to train an neural vocodder on. Steps to train an universal vocoder are like:</p>
<ul class="simple">
<li><p>Run NNSVS’ pre-processing for each database and combine them</p></li>
<li><p>Run vocoder training</p></li>
</ul>
<p>That’s it. Please check the recipes in <code class="docutils literal notranslate"><span class="pre">recipes/mixed</span></code> for example.</p>
</section>
<section id="training-on-mixed-speech-and-singing-databases">
<h3>Training on mixed speech and singing databases<a class="headerlink" href="#training-on-mixed-speech-and-singing-databases" title="Link to this heading"></a></h3>
<p>This should be easily implemented but not yet done by myself (r9y9). I may add code and docs for this in the future.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="optuna.html" class="btn btn-neutral float-left" title="Hyperparameter optimization with Optuna" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="train_usfgan.html" class="btn btn-neutral float-right" title="How to train uSFGAN/SiFiGAN vocoders" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Ryuichi Yamamoto.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>